{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41c417a-56d7-439d-827b-b0e47417d2bc",
   "metadata": {},
   "source": [
    "# <font color=\"blue\">AI 應用開發</font> <font size=4>by Enos Chou</font>\n",
    "\n",
    "<b>Focus</b>\n",
    "<ol>\n",
    "    <li>Google Cloud Speech-to-Text API</li>\n",
    "    <li>Google Cloud Vision API</li>\n",
    "    <li>Gemini API</li>\n",
    "    <li>Google Cloud Firestore for Vector</li>\n",
    "    <li>AI Agent ft. LangGraph + Gemini</li>\n",
    "    <li>Google Cloud Text-to-Speech API ft. Gemini</li>\n",
    "    <li>OpenAI API ft. Azure gpt-4o-mini-tts</li>\n",
    "</ol>\n",
    "\n",
    "<b>Language</b>\n",
    "<ul>\n",
    "    <li type=\"None\">Python 3.12</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68264913-01ca-4d07-bab9-48795d9d0689",
   "metadata": {},
   "source": [
    "## Google Cloud Speech-to-Text API\n",
    "\n",
    "<b>GCP Role</b>\n",
    "<ul>\n",
    "    <li type=\"None\"><font color=\"gray\">(optional) Storage 物件使用者</font></li>\n",
    "</ul>\n",
    "\n",
    "<b>Dependencies</b>\n",
    "<ul>\n",
    "    <li type=\"None\">google-cloud-speech</li>\n",
    "    <li type=\"None\"><font color=\"gray\">(optional) google-cloud-storage</font></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42178ae4-cf2f-4fd6-a0c6-6839f5560aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeb36a1-7cf0-4711-ab7d-7c632fb200c9",
   "metadata": {},
   "source": [
    "### 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f78e0a-fc7f-4157-87b3-abbcce31326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 1: init by assignment\n",
    "\n",
    "from google.cloud import speech\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "\n",
    "speech_client = speech.SpeechClient.from_service_account_json(YOUR_SERVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378714a-698e-4c45-8e57-144451f60ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2: init by environment\n",
    "\n",
    "import os\n",
    "\n",
    "from google.cloud import speech\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "speech_client = speech.SpeechClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55993a63-1042-4345-9e29-3f3a171fad55",
   "metadata": {},
   "source": [
    "### 準備音檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3dc8de-6a16-4fe1-b863-713acdfc0099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 1: one-shot upload \n",
    "\n",
    "YOUR_AUDIO = 'YOUR_AUDIO'\n",
    "\n",
    "# prepare audio content\n",
    "with open(YOUR_AUDIO, 'rb') as f:\n",
    "    content = f.read()\n",
    "audio = speech.RecognitionAudio(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c68ff5-eacf-44a1-9b9b-034dacb1c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2: through GCS\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "YOUR_BUCKET = 'YOUR_BUCKET'\n",
    "YOUR_AUDIO = 'YOUR_AUDIO'\n",
    "\n",
    "# prepare audio content\n",
    "storage_client = storage.Client.from_service_account_json(YOUR_SERVICE)\n",
    "bucket = storage_client.bucket(YOUR_BUCKET)\n",
    "bucket.blob(YOUR_AUDIO).upload_from_filename(YOUR_AUDIO)\n",
    "uri = f'gs://{YOUR_BUCKET}/{YOUR_AUDIO}'\n",
    "audio = speech.RecognitionAudio(uri=uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5361333-8915-4f48-a10b-317183393480",
   "metadata": {},
   "source": [
    "### recognize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75f080-cad4-4602-92e4-5cdce66e8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript the audio\n",
    "config = speech.RecognitionConfig(\n",
    "    encoding=speech.RecognitionConfig.AudioEncoding.MP3,  # omit this line if WAV\n",
    "    sample_rate_hertz=44100,\n",
    "    audio_channel_count=2,  # take care, default is 1\n",
    "    language_code=\"zh-TW\",\n",
    "    max_alternatives=10\n",
    ")\n",
    "response = speech_client.recognize(config=config, audio=audio)\n",
    "\n",
    "# get response\n",
    "for r in response.results:\n",
    "    print(f'{r.alternatives[0].transcript}, {r.alternatives[0].confidence:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417d291c-4b16-47c9-90ac-0302923930ab",
   "metadata": {},
   "source": [
    "## Google Cloud Vision API\n",
    "\n",
    "<b>GCP Role</b>\n",
    "<ul>\n",
    "    <li type=\"None\"><font color=\"gray\">(optional) Storage 物件使用者</font></li>\n",
    "</ul>\n",
    "\n",
    "<b>Dependencies</b>\n",
    "<ul>\n",
    "    <li type=\"None\">google-cloud-vision</li>\n",
    "    <li type=\"None\"><font color=\"gray\">(optional) google-cloud-storage</font></li>\n",
    "    <li type=\"None\"><font color=\"gray\">(optional) matplotlib</font></li>\n",
    "    <li type=\"None\"><font color=\"gray\">(optional) pillow</font></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324d25b-a587-4d73-97c6-5c6223066c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-vision matplotlib pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ce66c-200a-4bec-a7b2-c54e2f3dfb49",
   "metadata": {},
   "source": [
    "### 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d9ca4-52fd-4a2c-8ae4-78139f5796f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 1: init by assignment\n",
    "\n",
    "from google.cloud import vision\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "\n",
    "client = vision.ImageAnnotatorClient.from_service_account_json(YOUR_SERVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f5336f-05ef-4823-bd41-7165324cf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2: init by environment\n",
    "\n",
    "import os\n",
    "\n",
    "from google.cloud import vision\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243b91d-c509-4570-98cd-ad27f983c38d",
   "metadata": {},
   "source": [
    "### 準備圖檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c54e57-a51a-4416-b39c-76db6792e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 1: one-shot upload\n",
    "\n",
    "YOUR_PIC = 'YOUR_PIC'\n",
    "\n",
    "with open(YOUR_PIC, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "image = vision.Image(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a03286-2d09-4e47-809c-8af0378d176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case 2: through GCS\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "YOUR_BUCKET = 'YOUR_BUCKET'\n",
    "YOUR_PIC = 'YOUR_PIC'\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(YOUR_BUCKET)\n",
    "bucket.blob(YOUR_PIC).upload_from_filename(YOUR_PIC)\n",
    "image_uri = f'gs://{YOUR_BUCKET}/{YOUR_PIC}'\n",
    "source = vision.ImageSource(image_uri=image_uri)\n",
    "image = vision.Image(source=source)\n",
    "#image = vision.Image()\n",
    "#image.source.image_uri = image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca00b98-415d-486f-ba02-6da6fa4e220a",
   "metadata": {},
   "source": [
    "### label_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f332b2-775c-4ad5-8306-81f9c9749194",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.label_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a35ab-2d58-42bd-b2b2-b0a095708945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "for label in response.label_annotations:\n",
    "    print(f'{label.description}, {label.score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e2cb2-3994-412e-8357-7a2d5e4682fb",
   "metadata": {},
   "source": [
    "### face_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91367c21-3672-45e1-838c-55a038f00a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.face_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15d363-893a-4c73-b3ae-a5d9d545f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "\n",
    "plt.imshow(im)\n",
    "\n",
    "for face in response.face_annotations:\n",
    "    vs = [{'v': face.bounding_poly.vertices, 'c': 'red'},\n",
    "          {'v': face.fd_bounding_poly.vertices, 'c': 'blue'}]\n",
    "    for vd in vs:\n",
    "        a = [(v.x, v.y) for v in vd['v']]\n",
    "        a.append(a[0])\n",
    "        x, y = zip(*a)\n",
    "        plt.plot(x, y, color=vd['c'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for face in response.face_annotations:\n",
    "    print(f'joy: {face.joy_likelihood.name}')\n",
    "    print(f'sorrow: {face.sorrow_likelihood.name}')\n",
    "    print(f'anger: {face.anger_likelihood.name}')\n",
    "    print(f'surprise: {face.surprise_likelihood.name}')\n",
    "    print(f'under_exposed: {face.under_exposed_likelihood.name}')\n",
    "    print(f'blurred: {face.blurred_likelihood.name}')\n",
    "    print(f'headwear: {face.headwear_likelihood.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d075585-2563-421f-9d02-06ddc0255478",
   "metadata": {},
   "source": [
    "### text_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f58a25-f917-4b2b-86d6-357c597e94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.text_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1963ff-1867-4702-8708-c517c2ad4132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "\n",
    "plt.imshow(im)\n",
    "\n",
    "for text in response.text_annotations:\n",
    "    print(text.description)\n",
    "    a = [(v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    a.append(a[0])\n",
    "    x, y = zip(*a)\n",
    "    plt.plot(x, y, color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d1b21-8027-49f7-afc2-21e60342ee2c",
   "metadata": {},
   "source": [
    "### document_text_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c2606-a519-4401-83e8-103a5824546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.document_text_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e0892-3a27-4a65-bbf9-de4bd09f2b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "\n",
    "plt.imshow(im)\n",
    "\n",
    "for text in response.text_annotations:\n",
    "    print(text.description)\n",
    "    a = [(v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    a.append(a[0])\n",
    "    x, y = zip(*a)\n",
    "    plt.plot(x, y, color='blue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbaf5f-eecd-4997-a572-31ae9cddde5a",
   "metadata": {},
   "source": [
    "### object_localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679a411b-906f-417d-a07d-bc589255112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.object_localization(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9510a7-d344-4b3d-a03e-6e0262c2e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "\n",
    "plt.imshow(im)\n",
    "\n",
    "w, h = im.size\n",
    "for obj in response.localized_object_annotations:\n",
    "    desc = f'{obj.name}: {obj.score:.2f}'\n",
    "    a = [(v.x * w, v.y * h) for v in obj.bounding_poly.normalized_vertices]\n",
    "    a.append(a[0])\n",
    "    x, y = zip(*a)\n",
    "    plt.plot(x, y, color='blue')\n",
    "    plt.text(x[0], y[0], desc, color='yellow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631ebe05-990b-42ca-86f7-fcddc72da90f",
   "metadata": {},
   "source": [
    "### web_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aa9bfc-65ef-4a73-a36c-378ffea64065",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.web_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd54e4c-a1b3-400d-9255-b1bfacc925e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "for x in response.web_detection.web_entities:\n",
    "    print(x.description)\n",
    "for x in response.web_detection.visually_similar_images :\n",
    "    print(x.url)\n",
    "for x in response.web_detection.best_guess_labels:\n",
    "    print(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def9b0e-3f29-4901-bd50-77d0358ee26c",
   "metadata": {},
   "source": [
    "### landmark_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae946d-5894-4956-a2e2-a0a1ca669f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot upload\n",
    "\n",
    "YOUR_PIC = 'YOUR_PIC'\n",
    "\n",
    "with open(YOUR_PIC, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "image = vision.Image(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe8423-1d8d-42fd-b686-be376bfd3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.landmark_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc0d19-683e-4e14-92f2-dc7222b8f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "\n",
    "plt.imshow(im)\n",
    "\n",
    "for text in response.landmark_annotations:\n",
    "    desc = f'{text.description} @ ' \\\n",
    "           f'({text.locations[0].lat_lng.latitude}, {text.locations[0].lat_lng.longitude})'\n",
    "    print(desc)\n",
    "    a = [(v.x, v.y) for v in text.bounding_poly.vertices]\n",
    "    a.append(a[0])\n",
    "    x, y = zip(*a)\n",
    "    plt.plot(x, y, color='blue')\n",
    "    plt.text(x[0], y[0], desc, color='yellow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0200cdfe-3518-4cf2-8d8f-85480cfe7621",
   "metadata": {},
   "source": [
    "### logo_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7978c1b-462b-4073-a960-6a1e8c09ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot upload\n",
    "\n",
    "YOUR_PIC = 'YOUR_PIC'\n",
    "\n",
    "with open(YOUR_PIC, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "image = vision.Image(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743aee1-ffa7-4b21-aca8-ff2de108e385",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.logo_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85338f4-bead-4049-8a1b-0c2eaeb42313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "\n",
    "plt.imshow(im)\n",
    "\n",
    "for logo in response.logo_annotations :\n",
    "    print(logo.description)\n",
    "    a = [(v.x, v.y) for v in logo.bounding_poly.vertices]\n",
    "    a.append(a[0])\n",
    "    x, y = zip(*a)\n",
    "    plt.plot(x, y, color='blue')\n",
    "    plt.text(x[0], y[0], logo.description, color='yellow')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf3e8b-5512-46fc-a611-e7ecd1582844",
   "metadata": {},
   "source": [
    "### safe_search_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e481a-a7c6-413c-a7c1-04f2b163dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-shot upload\n",
    "\n",
    "YOUR_PIC = 'YOUR_PIC'\n",
    "\n",
    "with open(YOUR_PIC, 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "image = vision.Image(content=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088204f-15f7-48db-a4b0-9f7741a0dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.safe_search_detection(image=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959c18c-f9b8-4e82-8e55-28f324fdabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "im = Image.open(YOUR_PIC)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "\n",
    "print('adult:', response.safe_search_annotation.adult.name)\n",
    "print('spoof:', response.safe_search_annotation.spoof.name)\n",
    "print('medical:', response.safe_search_annotation.medical.name)\n",
    "print('violence:', response.safe_search_annotation.violence.name)\n",
    "print('racy:', response.safe_search_annotation.racy.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afbb91-2e01-4abc-b36d-e2219e5541a7",
   "metadata": {},
   "source": [
    "## Gemini API\n",
    "\n",
    "<b>GCP Role</b>\n",
    "<ul>\n",
    "    <li type=\"None\">Vertex AI 使用者</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dependency</b>\n",
    "<ul>\n",
    "    <li type=\"None\">google-genai</li>\n",
    "</ul>\n",
    "\n",
    "<b>Available MIME Types</b>\n",
    "\n",
    "<i>Document</i> <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/document-understanding#document-requirements\">[ref]</a>\n",
    "<ul><tt>\n",
    "    <li type=\"None\">application/pdf</li>\n",
    "    <li type=\"None\">text/plain</li>\n",
    "</tt></ul>\n",
    "\n",
    "<i>Audio</i> <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/audio-understanding#audio-requirements\">[ref]</a>\n",
    "<ul><tt>\n",
    "    <li type=\"None\">audio/aac</li>\n",
    "    <li type=\"None\">audio/flac</li>\n",
    "    <li type=\"None\">audio/mp3</li>\n",
    "    <li type=\"None\">audio/m4a</li>\n",
    "    <li type=\"None\">audio/mpeg</li>\n",
    "    <li type=\"None\">audio/mpga</li>\n",
    "    <li type=\"None\">audio/mp4</li>\n",
    "    <li type=\"None\">audio/opus</li>\n",
    "    <li type=\"None\">audio/pcm</li>\n",
    "    <li type=\"None\">audio/wav</li>\n",
    "    <li type=\"None\">audio/webm</li>\n",
    "</tt></ul>\n",
    "\n",
    "<i>Image</i> <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/image-understanding#image-requirements\">[ref]</a>\n",
    "<ul><tt>\n",
    "    <li type=\"None\">image/png</li>\n",
    "    <li type=\"None\">image/jpeg</li>\n",
    "    <li type=\"None\">image/webp</li>\n",
    "</tt></ul>\n",
    "\n",
    "<i>Video</i> <a href=\"https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding#video-requirements\">[ref]</a>\n",
    "<ul><tt>\n",
    "    <li type=\"None\">video/x-flv</li>\n",
    "    <li type=\"None\">video/quicktime</li>\n",
    "    <li type=\"None\">video/mpeg</li>\n",
    "    <li type=\"None\">video/mpegps</li>\n",
    "    <li type=\"None\">video/mpg</li>\n",
    "    <li type=\"None\">video/mp4</li>\n",
    "    <li type=\"None\">video/webm</li>\n",
    "    <li type=\"None\">video/wmv</li>\n",
    "    <li type=\"None\">video/3gpp</li>\n",
    "</tt></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56022e8f-0ea1-4aa0-92b9-84cdbad7b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4e8dc-5ef6-4992-9b3a-b1cdf50ab379",
   "metadata": {},
   "source": [
    "### 1. 車牌辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad390e-223b-4d4c-82a1-3319adf49ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig\n",
    "\n",
    "\n",
    "YOUR_JPG = 'YOUR_JPG'\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=100,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# prompt\n",
    "prompt = Part.from_text(text=YOUR_PROMPT)\n",
    "\n",
    "# data\n",
    "with open(YOUR_JPG, 'rb') as f:\n",
    "    data = f.read()\n",
    "data = Part.from_bytes(data=data, mime_type='image/jpeg')\n",
    "contents = [Content(role='user', parts=[prompt, data])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "print(r.text)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba22f364-bc6f-446a-9168-2eb2e962d048",
   "metadata": {},
   "source": [
    "### 2. 健檢報告分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c984e927-3a75-4936-978c-ebadb1915059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig\n",
    "\n",
    "\n",
    "YOUR_JPG = 'YOUR_JPG'\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=2000,\n",
    "    temperature=0,\n",
    "    response_mime_type='application/json'\n",
    ")\n",
    "\n",
    "# prompt\n",
    "prompt = Part.from_text(text=YOUR_PROMPT)\n",
    "\n",
    "# data\n",
    "with open(YOUR_JPG, 'rb') as f:\n",
    "    data = f.read()\n",
    "data = Part.from_bytes(data=data, mime_type='image/jpeg')\n",
    "contents = [Content(role='user', parts=[prompt, data])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "pprint(json.loads(r.text))\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a7bd99-82ec-407e-8dbb-0b99dec58f47",
   "metadata": {},
   "source": [
    "### 3. 文件摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437e5eb-61a5-435f-8e5d-7a0cb2e93f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig\n",
    "\n",
    "\n",
    "YOUR_PDF = 'YOUR_PDF'\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=2000,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "# prompt\n",
    "prompt = Part.from_text(text=YOUR_PROMPT)\n",
    "\n",
    "# data\n",
    "with open(YOUR_PDF, 'rb') as f:\n",
    "    data = f.read()\n",
    "data = Part.from_bytes(data=data, mime_type='application/pdf')\n",
    "contents = [Content(role='user', parts=[prompt, data])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "print(r.text)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25339169-cc9a-4f66-bc10-ce76d2dffb6e",
   "metadata": {},
   "source": [
    "### 4. 影評歸納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c888e3-7219-4135-ad27-66918dce9826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# prompt\n",
    "prompt = Part.from_text(text=YOUR_PROMPT)\n",
    "\n",
    "# data\n",
    "data = input('請輸入影評：')\n",
    "data = Part.from_text(text=data)\n",
    "contents = [Content(role='user', parts=[prompt, data])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "pprint(json.loads(r.text.strip('json`\\n')))\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161890e2-51b9-4367-8565-60ec0914766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "YOUR_RESPONSE_SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\"type\": \"string\"},\n",
    "        \"abstract\": {\"type\": \"string\"},\n",
    "        \"review\": {\"type\": \"string\", \"enum\": [\"正評\", \"中立\", \"負評\"]}\n",
    "    },\n",
    "    \"required\": [\"name\", \"abstract\", \"review\"]\n",
    "}\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=1024,\n",
    "    temperature=0,\n",
    "    response_mime_type='application/json',\n",
    "    response_schema=YOUR_RESPONSE_SCHEMA\n",
    ")\n",
    "\n",
    "# prompt\n",
    "prompt = Part.from_text(text=YOUR_PROMPT)\n",
    "\n",
    "# data\n",
    "data = input('請輸入影評：')\n",
    "data = Part.from_text(text=data)\n",
    "contents = [Content(role='user', parts=[prompt, data])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "pprint(r.parsed)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015cb8a5-e81f-42eb-8f45-c5b5abda40ee",
   "metadata": {},
   "source": [
    "### 5. 文字聊天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e37373d-4eb5-4883-bff8-4561fdf306d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_SYSTEM_INSTRUCTION = 'YOUR_SYSTEM_INSTRUCTION'\n",
    "YOUR_SAFETY_SETTINGS = [\n",
    "    {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'}\n",
    "]\n",
    "YOUR_CHAT = 'YOUR_CHAT'\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=1024,\n",
    "    temperature=1.5,\n",
    "    system_instruction=YOUR_SYSTEM_INSTRUCTION,\n",
    "    safety_settings=YOUR_SAFETY_SETTINGS\n",
    ")\n",
    "\n",
    "# prompt\n",
    "chat = Part.from_text(text=YOUR_CHAT)\n",
    "\n",
    "# data\n",
    "contents = [Content(role='user', parts=[chat])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "print(r.text)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865b895-3248-4aa9-b867-f8353e100868",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_CHAT = 'YOUR_CHAT'\n",
    "\n",
    "# prompt\n",
    "chat = Part.from_text(text=YOUR_CHAT)\n",
    "\n",
    "# data\n",
    "contents = [Content(role='user', parts=[chat])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "print(r.text)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef810c-1c88-49bd-ae06-8c6f3c3eb9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_SYSTEM_INSTRUCTION = '''YOUR_SYSTEM_INSTRUCTION'''\n",
    "YOUR_SAFETY_SETTINGS = [\n",
    "    {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'}\n",
    "]\n",
    "YOUR_CHAT = 'YOUR_CHAT'\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=1024,\n",
    "    temperature=1.5,\n",
    "    system_instruction=YOUR_SYSTEM_INSTRUCTION,\n",
    "    safety_settings=YOUR_SAFETY_SETTINGS\n",
    ")\n",
    "\n",
    "# prompt\n",
    "chat = Part.from_text(text=YOUR_CHAT)\n",
    "\n",
    "# data\n",
    "contents = [Content(role='user', parts=[chat])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "print(r.text)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64b2ff-1d6b-4d98-b25e-cbe570abe0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_CHAT = 'YOUR_CHAT'\n",
    "\n",
    "# prompt\n",
    "previous_chat, previous_res = chat, Part.from_text(text=r.text)\n",
    "chat = Part.from_text(text=YOUR_CHAT)\n",
    "\n",
    "# data\n",
    "contents = [Content(role='user', parts=[previous_chat]),\n",
    "            Content(role='model', parts=[previous_res]),\n",
    "            Content(role='user', parts=[chat])\n",
    "           ]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "print(r.text)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98061c2-f4e8-48c3-875e-02a1a7ffe0c5",
   "metadata": {},
   "source": [
    "### 6. 聊天查詢真實天氣\n",
    "ref 1: https://ai.google.dev/gemini-api/docs/function-calling?hl=zh-tw<br>\n",
    "ref 2: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26f27d-ac0f-4a45-8462-4866716c095e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step One\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import Part, Content, GenerateContentConfig, FunctionDeclaration, Tool\n",
    "import wea\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_SYSTEM_INSTRUCTION = '你專注於查詢並回報台灣各地天氣，總是引導客戶詢問正確的地理位置，藉由工具計算的結果回覆天氣資訊'\n",
    "YOUR_SAFETY_SETTINGS = [\n",
    "    {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_NONE'},\n",
    "    {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_NONE'}\n",
    "]\n",
    "YOUR_CHAT = 'YOUR_CHAT'\n",
    "\n",
    "# initialization\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)\n",
    "\n",
    "# function declaration\n",
    "wea_fndec = FunctionDeclaration(\n",
    "    name='get_taiwan_weather',\n",
    "    description=\"Get current weather of a given location in Taiwan\",\n",
    "    parameters={\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\"site\": {\"type\": \"string\", \"description\": \"Location of Taiwan\", \"enum\": [\"臺北\", \"苗栗\"]}},\n",
    "        \"required\": [\"site\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# tools\n",
    "tools = Tool(function_declarations=[wea_fndec])\n",
    "\n",
    "# parameters\n",
    "config = GenerateContentConfig(\n",
    "    max_output_tokens=1024,\n",
    "    temperature=1.5,\n",
    "    system_instruction=YOUR_SYSTEM_INSTRUCTION,\n",
    "    safety_settings=YOUR_SAFETY_SETTINGS,\n",
    "    tools=[tools]\n",
    ")\n",
    "\n",
    "# prompt\n",
    "chat = Part.from_text(text=YOUR_CHAT)\n",
    "\n",
    "# data\n",
    "contents = [Content(role='user', parts=[chat])]\n",
    "\n",
    "# call model\n",
    "start = time()\n",
    "\n",
    "r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "\n",
    "# output\n",
    "print(f'{time()-start:.3f}s elapsed')\n",
    "if r.function_calls:\n",
    "    for f in r.function_calls:\n",
    "        print(f)\n",
    "else:\n",
    "    print(r.text)\n",
    "print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee373a7-46be-4208-bcaf-7076202c2e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Two\n",
    "\n",
    "if r.function_calls:\n",
    "    # data\n",
    "    for fn in r.function_calls:\n",
    "        if fn.name == 'get_taiwan_weather':\n",
    "            wea_info = wea.tostr(wea.grab(**fn.args))\n",
    "            fn_response = Part.from_function_response(\n",
    "                name=fn.name,\n",
    "                response={'result': wea_info}\n",
    "            )\n",
    "            contents.append(Content(role=\"model\", parts=[Part(function_call=fn)]))\n",
    "            contents.append(Content(role=\"user\", parts=[fn_response]))\n",
    "    \n",
    "    # call model\n",
    "    start = time()\n",
    "    \n",
    "    r = genai_client.models.generate_content(model=YOUR_MODEL, contents=contents, config=config)\n",
    "    \n",
    "    # output\n",
    "    print(f'{time()-start:.3f}s elapsed')\n",
    "    print(r.text)\n",
    "    print(r.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d208ade-3421-4107-b4eb-9c57cb714491",
   "metadata": {},
   "source": [
    "## Google Cloud Firestore for Vector\n",
    "\n",
    "<b>GCP Role</b>\n",
    "<ul>\n",
    "    <li type=\"None\">Vertex AI 使用者</li>\n",
    "    <li type=\"None\">Cloud Datastore 使用者</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dependencies</b>\n",
    "<ul>\n",
    "    <li type=\"None\">google-genai</li>\n",
    "    <li type=\"None\">google-cloud-firestore</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce73dc32-6d8b-4bc8-94d9-f935d3c364bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-genai google-cloud-firestore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcba700-4c4f-4022-8a56-56df9bccaf9a",
   "metadata": {},
   "source": [
    "### 2. prepare the data and embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b786e374-5cb6-4542-8c37-fbe8e42c0326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_MODEL = 'gemini-embedding-001'  # 維度上限 3072\n",
    "YOUR_DIM = 768  # Firestore Vector 維度上限 2048\n",
    "YOUR_EMBEDDING_KEY = 'YOUR_EMBEDDING_KEY'\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd38fd5-2582-415f-b215-29ea59930927",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {'title': '植物辨識',\n",
    "     'text': '用 YOLO 與 CNN 技術訓練植物辨識 AI 模型，並且整合到 LINE Bot 以及 Android APP',\n",
    "     'author': ['John Wang', 'Jack Chang'],\n",
    "     'date': '20230603'},\n",
    "    {'title': '點名系統',\n",
    "     'text': '運用 dlib 人臉辨識模組實作點名系統，以 Windows Desktop 呈現',\n",
    "     'author': ['Jerry Lin', 'John Wang', 'Jim Hsiao'],\n",
    "     'date': '20230730'},\n",
    "    {'title': '跳舞機',\n",
    "     'text': '利用 MediaPipe 搭配 Tkinter 實作跨平台跳舞機，並以邊緣運算裝置進行螢幕輸出',\n",
    "     'author': ['Jackson Lu', 'James Chao'],\n",
    "     'date': '20240426'},\n",
    "    {'title': '理賠計算機',\n",
    "     'text': '搭配 Gemini 與 LINE Bot 實作保險理賠計算機',\n",
    "     'author': ['Jenny Wu', 'Jacob Chiu', 'Jim Hsiao'],\n",
    "     'date': '20250712'},\n",
    "    {'title': '資安檢測機器人',\n",
    "     'text': '混和 AI Agent 與 RAG 技術，實作全自動資安檢測機器人',\n",
    "     'author': ['Joe Li', 'Jim Hsiao'],\n",
    "     'date': '20251011'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c1e7b-363b-4e09-8382-19e878e31013",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = [d['text'] for d in data]\n",
    "r = genai_client.models.embed_content(\n",
    "    model=YOUR_MODEL,\n",
    "    contents=contents,\n",
    "    config=EmbedContentConfig(taskType='RETRIEVAL_DOCUMENT', outputDimensionality=YOUR_DIM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a9b46-ab04-48c1-a216-acac4708d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68862d90-0cee-44a4-aea6-d3cbe8ed4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [d | {'id': f'{i:03}', YOUR_EMBEDDING_KEY: Vector(e.values)} for i, (d, e) in enumerate(zip(data, r.embeddings))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21306b12-1c0c-4e11-8d31-99d2ed1483ac",
   "metadata": {},
   "source": [
    "### 3. store the data into Firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10cbbd-e429-4504-9ed3-4dc86bc4f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_DATABASE = 'YOUR_DATABASE'  # None 為預設 DB\n",
    "YOUR_COLLECTION = 'YOUR_COLLECTION'\n",
    "\n",
    "firestore_client = firestore.Client.from_service_account_json(YOUR_SERVICE, database=YOUR_DATABASE)\n",
    "collection = firestore_client.collection(YOUR_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d933da87-33c8-4bc9-b53d-8be797823aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in docs:\n",
    "    collection.document(d['id']).set(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702cc74-c6e1-482e-ac7a-820a63004eae",
   "metadata": {},
   "source": [
    "### 5. Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b57e66c-48ff-42c3-b0fd-ad13c94ecc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import EmbedContentConfig\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_REGION = 'YOUR_REGION'\n",
    "YOUR_MODEL = 'gemini-embedding-001'  # 維度上限 3072\n",
    "YOUR_DIM = 768  # Firestore Vector 維度上限 2048\n",
    "YOUR_EMBEDDING_KEY = 'YOUR_EMBEDDING_KEY'\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "genai_client = genai.Client(vertexai=True, location=YOUR_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbdda3-2a98-49df-94e5-aa0028a19806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "from google.cloud.firestore_v1.vector import Vector\n",
    "from google.cloud.firestore_v1.base_vector_query import DistanceMeasure\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_DATABASE = 'YOUR_DATABASE'  # None 為預設 DB\n",
    "YOUR_COLLECTION = 'YOUR_COLLECTION'\n",
    "YOUR_EMBEDDING_KEY = 'YOUR_EMBEDDING_KEY'\n",
    "\n",
    "firestore_client = firestore.Client.from_service_account_json(YOUR_SERVICE, database=YOUR_DATABASE)\n",
    "collection = firestore_client.collection(YOUR_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad3b9a-c9ad-419c-8681-69cece705b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_SEARCH = 'YOUR_SEARCH'\n",
    "\n",
    "r = genai_client.models.embed_content(\n",
    "    model=YOUR_MODEL,\n",
    "    contents=[YOUR_SEARCH],\n",
    "    config=EmbedContentConfig(taskType='RETRIEVAL_DOCUMENT', outputDimensionality=YOUR_DIM)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de883739-2c5c-444a-9f9e-34a560f8a026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 向量搜尋\n",
    "vector_search = collection.find_nearest(\n",
    "    vector_field=YOUR_EMBEDDING_KEY,\n",
    "    query_vector=Vector(r.embeddings[0].values),\n",
    "    #distance_measure=DistanceMeasure.EUCLIDEAN,  # 愈小愈接近\n",
    "    #distance_measure=DistanceMeasure.COSINE,  # 愈小愈接近\n",
    "    distance_measure=DistanceMeasure.DOT_PRODUCT,  # 愈大愈接近\n",
    "    limit=5,\n",
    "    distance_result_field='d'  # 保留距離欄位\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52168f-ab08-455c-9a44-b9b374ee8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混合搜尋\n",
    "vector_search = collection.where(\"author\", \"array_contains\", \"Jim Hsiao\").find_nearest(\n",
    "    vector_field=YOUR_EMBEDDING_KEY,\n",
    "    query_vector=Vector(r.embeddings[0].values),\n",
    "    #distance_measure=DistanceMeasure.EUCLIDEAN,  # 愈小愈接近\n",
    "    #distance_measure=DistanceMeasure.COSINE,  # 愈小愈接近\n",
    "    distance_measure=DistanceMeasure.DOT_PRODUCT,  # 愈大愈接近\n",
    "    limit=5,\n",
    "    distance_result_field='d'  # 保留距離欄位\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058a323-e339-4e84-9c53-65c677466a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest = [v.to_dict() for v in vector_search.get()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6415bc4f-6951-4b47-a6e1-7a71a95528c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint([(a['text'], a['d']) for a in nearest])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baafb092-2758-4744-91c9-a16ffd920ecb",
   "metadata": {},
   "source": [
    "## AI Agent ft. LangGraph + Gemini\n",
    "\n",
    "<b>GCP 啟用 API</b>\n",
    "<ul>\n",
    "    <li type=\"None\">Generative Language API or Gememi API</li>\n",
    "</ul>\n",
    "\n",
    "<b>GCP Role</b>\n",
    "<ul>\n",
    "    <li type=\"None\">Vertex AI 使用者</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dependencies</b>\n",
    "<ul>\n",
    "    <li type=\"None\">langgraph</li>\n",
    "    <li type=\"None\">langchain-google-genai</li>\n",
    "    <li type=\"None\">geopy</li>\n",
    "    <li type=\"None\">wea</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cb716a-abec-489f-af4d-c89fa52a19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph langchain-google-genai geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fa343-5991-43de-9f13-bfecb0ba1d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from geopy.geocoders import Nominatim\n",
    "import wea\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "llm = ChatGoogleGenerativeAI(model=YOUR_MODEL, temperature=0)\n",
    "geolocator = Nominatim(user_agent='user_agent')\n",
    "\n",
    "@tool\n",
    "def get_coordinates(location: str) -> dict[str, float] | str:\n",
    "    '''輸入地點名稱, 取得地點座標'''\n",
    "    if loc := geolocator.geocode(location):\n",
    "        return {'latitude': loc.latitude, 'longitude': loc.longitude}\n",
    "    return f\"找不到 {location}\"\n",
    "\n",
    "@tool\n",
    "def get_weather_by_location_coordinates(latitude: float, longitude: float) -> str:    \n",
    "    '''輸入地點座標, 取得天氣資訊'''\n",
    "    return wea.tostr(wea.grab((latitude, longitude)))\n",
    "\n",
    "\n",
    "tools = [\n",
    "    get_coordinates,\n",
    "    get_weather_by_location_coordinates\n",
    "]\n",
    "agent = create_react_agent(model=llm, tools=tools, prompt=YOUR_PROMPT)\n",
    "\n",
    "\n",
    "def ask(something: str) -> str:\n",
    "    '''ask AI Agent something\n",
    "    something - asking\n",
    "    return    - answer from AI Agent\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": something}]\n",
    "    result = agent.invoke({\"messages\": messages})\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    something = '擎天崗天氣如何？'\n",
    "    print(ask(something))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c964b-b487-411a-b30a-fd9f3a1a456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from geopy.geocoders import Nominatim\n",
    "import wea\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = YOUR_SERVICE\n",
    "llm = ChatGoogleGenerativeAI(model=YOUR_MODEL, temperature=0)\n",
    "geolocator = Nominatim(user_agent='user_agent')\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_coordinates(location: str) -> dict[str, float] | str:\n",
    "    '''輸入地點名稱, 取得地點座標'''\n",
    "    if loc := geolocator.geocode(location):\n",
    "        return {'latitude': loc.latitude, 'longitude': loc.longitude}\n",
    "    return f\"找不到 {location}\"\n",
    "\n",
    "@tool\n",
    "def get_weather_by_location_coordinates(latitude: float, longitude: float) -> str:    \n",
    "    '''輸入地點座標, 取得天氣資訊'''\n",
    "    return wea.tostr(wea.grab((latitude, longitude)))\n",
    "    \n",
    "\n",
    "@tool\n",
    "def get_weather_by_location_name(name: str) -> str:  \n",
    "    '''輸入地點名稱, 取得天氣資訊'''\n",
    "    return wea.tostr(wea.grab(name))\n",
    "\n",
    "tools = [\n",
    "    get_coordinates,\n",
    "    get_weather_by_location_coordinates,\n",
    "    get_weather_by_location_name\n",
    "]\n",
    "agent = create_react_agent(model=llm, tools=tools, prompt=YOUR_PROMPT)\n",
    "\n",
    "\n",
    "def ask(something: str) -> str:\n",
    "    '''ask AI Agent something\n",
    "    something - asking\n",
    "    return    - answer from AI Agent\n",
    "    '''\n",
    "    messages = [{\"role\": \"user\", \"content\": something}]\n",
    "    result = agent.invoke({\"messages\": messages})\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    something = '擎天崗天氣如何？'\n",
    "    print(ask(something))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "744fbb4c-38f5-4b0f-853f-7938af9e7011",
   "metadata": {},
   "source": [
    "## Google Cloud Text-to-Speech API ft. Gemini\n",
    "\n",
    "<b>GCP 啟用 API</b>\n",
    "<ul>\n",
    "    <li type=\"None\">Cloud Text-to-Speech API</li>\n",
    "    <li type=\"None\">Generative Language API or Gememi API</li>\n",
    "</ul>\n",
    "\n",
    "<b>GCP Role</b>\n",
    "<ul>\n",
    "    <li type=\"None\">Vertex AI 使用者</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dependencies</b>\n",
    "<ul>\n",
    "    <li type=\"None\">google-cloud-texttospecch</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99be963-8fce-456f-b27b-b9c627aef1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-texttospecch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab5e2f-84c5-4e83-b356-e7c137193023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import texttospeech\n",
    "\n",
    "\n",
    "YOUR_SERVICE = 'YOUR_SERVICE'\n",
    "YOUR_MODEL = 'YOUR_MODEL'  # ex: gemini-2.5-pro-tts (better), gemini-2.5-flash-tts\n",
    "YOUR_AUDIO = 'YOUR_AUDIO'\n",
    "YOUR_INPUT = 'YOUR_INPUT'\n",
    "YOUR_PROMPT = 'YOUR_PROMPT'\n",
    "YOUR_VOICE = 'YOUR_VOICE'  # ex: Sulafat\n",
    "\n",
    "\n",
    "tts_client = texttospeech.TextToSpeechClient.from_service_account_json(\n",
    "    YOUR_SERVICE\n",
    ")\n",
    "\n",
    "synthesis_input = texttospeech.SynthesisInput(text=YOUR_INPUT, prompt=YOUR_PROMPT)\n",
    "\n",
    "voice = texttospeech.VoiceSelectionParams(\n",
    "    language_code=\"cmn-tw\",  # cmn-tw: 台灣國語/ ja-JP: 日語/ en-US: 英語/ cmn-cn: 大陸語\n",
    "    name=YOUR_VOICE,\n",
    "    model_name=YOUR_MODEL\n",
    ")\n",
    "\n",
    "audio_config = texttospeech.AudioConfig(\n",
    "    audio_encoding=texttospeech.AudioEncoding.MP3  # 指定輸出格式\n",
    ")\n",
    "\n",
    "response = tts_client.synthesize_speech(\n",
    "    input=synthesis_input,\n",
    "    voice=voice,\n",
    "    audio_config=audio_config\n",
    ")\n",
    "\n",
    "with open(YOUR_AUDIO, 'wb') as f:\n",
    "    f.write(response.audio_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4abb6b-cf59-4454-bc61-51d18db6e77a",
   "metadata": {},
   "source": [
    "## OpenAI API ft. Azure gpt-4o-mini-tts\n",
    "\n",
    "<b>Azure AI Foundry 部署</b>\n",
    "<ul>\n",
    "    <li type=\"None\">gpt-4o-mini-tts</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dependencies</b>\n",
    "<ul>\n",
    "    <li type=\"None\">openai</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fa746-672d-4515-b90b-d0cd304a0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2eb8ad-da65-43e9-8e30-2e433481cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "YOUR_AZURE_ENDPOINT = 'YOUR_AZURE_ENDPOINT'  # ex: https://xxx.openai.azure.com/\n",
    "YOUR_API_KEY = 'YOUR_API_KEY'\n",
    "YOUR_API_VERSION = 'YOUR_API_VERSION'  # ex: 2025-03-01-preview\n",
    "YOUR_MODEL = 'YOUR_MODEL'  # ex: gpt-4o-mini-tts\n",
    "YOUR_INPUT = 'YOUR_INPUT'  # ex: 今天都在下雨，很煩\n",
    "YOUR_INSTRUCTIONS = 'YOUR_INSTRUCTIONS'  # ex: 傷心的語調\n",
    "YOUR_VOICE = 'YOUR_VOICE'  # ex: nova\n",
    "YOUR_FORMAT = 'YOUR_FORMAT'  # ex: mp3\n",
    "YOUR_AUDIO = 'YOUR_FILE'  # tts.mp3\n",
    "\n",
    "\n",
    "model = AzureOpenAI(\n",
    "    api_key=YOUR_API_KEY,\n",
    "    azure_endpoint=YOUR_AZURE_ENDPOINT,\n",
    "    api_version=YOUR_API_VERSION\n",
    ")\n",
    "\n",
    "r = model.audio.speech.create(\n",
    "    model=YOUR_MODEL,\n",
    "    input=YOUR_INPUT,\n",
    "    instructions=YOUR_INSTRUCTIONS,\n",
    "    voice=YOUR_VOICE,\n",
    "    response_format=YOUR_FORMAT\n",
    ")\n",
    "\n",
    "r.write_to_file(YOUR_AUDIO, 'wb')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
